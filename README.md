# Mistral-7B
mistralai/Mistral-7B-Instruct-v0.2 모델을 사용해 간단하게 챗봇 만들어보기.
# 📚 프로젝트 소개
이 프로젝트는 OpenChat 3.5 모델을 기반으로
간단한 프론트엔드 입력창을 통해 자유롭게 질문을 입력하고,
모델이 자연어로 답변을 생성하는 텍스트 생성 서비스입니다.

FastAPI를 이용해 경량 서버를 구축하고,
Web 페이지(HTML + JS)로 사용자 인터페이스를 제공합니다.

모델 처리 속도 최적화를 위해 GPU 활용과 메모리 최적화를 적용하여,
빠른 응답성과 경량화된 텍스트 생성을 동시에 목표로 합니다.

# 🧠 사용 모델: OpenChat 3.5 (openchat/openchat-3.5-1210)
본 프로젝트에서 사용한 모델은 HuggingFace에서 제공하는
openchat/openchat-3.5-1210 버전입니다.

OpenChat 3.5는 GPT 아키텍처 기반의 **대형 언어 모델(LLM)**로,
특히 영어와 중국어에 대해 매우 뛰어난 자연어 이해 및 생성 능력을 보이는 모델입니다.

모델 개발진은 주로 중국계 개발자 그룹으로 구성되어 있어,
중국어 및 영어 데이터셋을 중심으로 학습되었습니다.
이로 인해 한국어 입력에 대해서는 다소 부자연스러운 문장 구성이나 의미 왜곡이 발생할 수 있습니다.

그럼에도 불구하고,
본 프로젝트에서는 "영어 및 중국어 기반 다국어 확장성"을 고려하여,
한국어 특화 모델 대신 OpenChat 3.5를 채택하였습니다.
(추후 서비스 확장 시, 중국어·영어 기반 지원을 우선 확보하기 위함입니다.)

# ✨ 모델 특징 (코드 기반 설명)
OpenChat 3.5를 실사용하기 위해 다음과 같은 세부 설정 및 최적화가 적용되었습니다:

| 항목 | 설명 |
|:---|:---|
| 모델 로딩 최적화 | load_in_4bit=True 옵션을 사용하여 모델을 4bit 양자화해 메모리 사용량을 대폭 절감했습니다. |
| GPU 우선 배치 | device_map="auto" 설정을 통해 가능한 레이어를 GPU에 우선 배치하여 추론 속도를 최적화했습니다. |
| 연산 최적화 타입 | torch_dtype=torch.float16 설정을 통해 Half-Precision (FP16) 연산으로 처리 속도를 가속화했습니다. |
| 메모리 오프로딩 지원 | GPU 메모리가 부족할 경우 일부 레이어를 CPU 또는 디스크(offload_folder)로 오프로드하여 시스템 안정성을 확보했습니다. |
| 추론 설정 최적화 | 답변 생성 시 temperature=0.7, max_new_tokens=1024를 설정하여 자연스러운 문장 생성과 메모리 절약을 균형 있게 조정했습니다. |


# 📈 추가 요약
장점:

영어·중국어 탁월한 성능

고속 응답(4bit 경량화 + GPU 활용)

대규모 문맥 이해 가능

단점:

한국어 최적화는 미흡 (다소 어색하거나 부정확한 문장 생성 가능)

✅ 요약 문구
한국어 완성도보다는, 영어·중국어 대응성과 모델 확장성 확보를 중시하여 OpenChat 3.5를 채택했습니다.
